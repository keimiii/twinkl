{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Judge Labeling - LLM-as-Judge for Alignment Scores\n",
        "\n",
        "This notebook implements the **Judge** component of the VIF pipeline. It takes journal entries (from synthetic personas or real users) and labels them with per-dimension alignment scores using an LLM.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The Judge assigns a **3-point categorical alignment score** for each of the 10 Schwartz value dimensions:\n",
        "- **-1 (Misaligned):** Entry actively conflicts with this value\n",
        "- **0 (Neutral):** Entry is irrelevant to the value or maintains status quo  \n",
        "- **+1 (Aligned):** Entry actively supports this value\n",
        "\n",
        "These labels serve as ground-truth training targets for the Critic MLP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import yaml\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from jinja2 import Template\n",
        "from openai import AsyncOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Check for API Key\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"WARNING: OPENAI_API_KEY not found in environment variables.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configs loaded successfully.\n",
            "Schwartz Values (10): ['Self-Direction', 'Stimulation', 'Hedonism', 'Achievement', 'Power', 'Security', 'Conformity', 'Tradition', 'Benevolence', 'Universalism']\n"
          ]
        }
      ],
      "source": [
        "# Configuration Loading\n",
        "CONFIG_PATH = Path(\"config/synthetic_data.yaml\")\n",
        "if not CONFIG_PATH.exists():\n",
        "    CONFIG_PATH = Path(\"../config/synthetic_data.yaml\")\n",
        "\n",
        "SCHWARTZ_VALUES_PATH = Path(\"config/schwartz_values.yaml\")\n",
        "if not SCHWARTZ_VALUES_PATH.exists():\n",
        "    SCHWARTZ_VALUES_PATH = Path(\"../config/schwartz_values.yaml\")\n",
        "\n",
        "\n",
        "def load_config(path: str | Path) -> dict:\n",
        "    with open(path, \"r\") as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "\n",
        "config = load_config(CONFIG_PATH)\n",
        "schwartz_config = load_config(SCHWARTZ_VALUES_PATH)\n",
        "\n",
        "# Define Schwartz value order (must match across all components)\n",
        "SCHWARTZ_VALUE_ORDER = [\n",
        "    \"Self-Direction\",\n",
        "    \"Stimulation\", \n",
        "    \"Hedonism\",\n",
        "    \"Achievement\",\n",
        "    \"Power\",\n",
        "    \"Security\",\n",
        "    \"Conformity\",\n",
        "    \"Tradition\",\n",
        "    \"Benevolence\",\n",
        "    \"Universalism\"\n",
        "]\n",
        "\n",
        "print(\"Configs loaded successfully.\")\n",
        "print(f\"Schwartz Values ({len(SCHWARTZ_VALUE_ORDER)}): {SCHWARTZ_VALUE_ORDER}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Models\n",
        "\n",
        "Defining structured outputs for the Judge's alignment labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AlignmentLabel(BaseModel):\n",
        "    \"\"\"Per-dimension alignment scores from the Judge.\"\"\"\n",
        "    \n",
        "    # Each value dimension gets a score in {-1, 0, +1}\n",
        "    self_direction: int = Field(ge=-1, le=1, description=\"Self-Direction alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    stimulation: int = Field(ge=-1, le=1, description=\"Stimulation alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    hedonism: int = Field(ge=-1, le=1, description=\"Hedonism alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    achievement: int = Field(ge=-1, le=1, description=\"Achievement alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    power: int = Field(ge=-1, le=1, description=\"Power alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    security: int = Field(ge=-1, le=1, description=\"Security alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    conformity: int = Field(ge=-1, le=1, description=\"Conformity alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    tradition: int = Field(ge=-1, le=1, description=\"Tradition alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    benevolence: int = Field(ge=-1, le=1, description=\"Benevolence alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    universalism: int = Field(ge=-1, le=1, description=\"Universalism alignment: -1=misaligned, 0=neutral, +1=aligned\")\n",
        "    \n",
        "    def to_vector(self) -> list[int]:\n",
        "        \"\"\"Convert to ordered vector matching SCHWARTZ_VALUE_ORDER.\"\"\"\n",
        "        return [\n",
        "            self.self_direction,\n",
        "            self.stimulation,\n",
        "            self.hedonism,\n",
        "            self.achievement,\n",
        "            self.power,\n",
        "            self.security,\n",
        "            self.conformity,\n",
        "            self.tradition,\n",
        "            self.benevolence,\n",
        "            self.universalism\n",
        "        ]\n",
        "\n",
        "\n",
        "# JSON schema for strict mode\n",
        "ALIGNMENT_LABEL_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"additionalProperties\": False,\n",
        "    \"properties\": {\n",
        "        \"self_direction\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "        \"stimulation\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "        \"hedonism\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "        \"achievement\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "        \"power\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "        \"security\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "        \"conformity\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "        \"tradition\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "        \"benevolence\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "        \"universalism\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 1},\n",
        "    },\n",
        "    \"required\": [\n",
        "        \"self_direction\", \"stimulation\", \"hedonism\", \"achievement\", \"power\",\n",
        "        \"security\", \"conformity\", \"tradition\", \"benevolence\", \"universalism\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "ALIGNMENT_LABEL_RESPONSE_FORMAT = {\n",
        "    \"type\": \"json_schema\",\n",
        "    \"name\": \"AlignmentLabel\",\n",
        "    \"schema\": ALIGNMENT_LABEL_SCHEMA,\n",
        "    \"strict\": True,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample rubric context (first 500 chars):\n",
            "\n",
            "### Self-Direction\n",
            "**Core Motivation:** The fundamental drive to think for oneself, make one's own choices, and resist external control. Self-Direction-oriented individuals feel most alive when they are authoring their own path, even if that path is harder or less conventional.\n",
            "\n",
            "**Key Behaviors (Aligned):**\n",
            "- Resists being told what to do; bristles at micromanagement or rigid hierarchies\n",
            "- Seeks out problems that require novel solutions rather than following established procedures\n",
            "- Makes caree...\n"
          ]
        }
      ],
      "source": [
        "def build_value_rubric_context(schwartz_config: dict) -> str:\n",
        "    \"\"\"Build a concise rubric for each Schwartz value to guide the Judge.\"\"\"\n",
        "    context_parts = []\n",
        "    \n",
        "    for value_name in SCHWARTZ_VALUE_ORDER:\n",
        "        if value_name not in schwartz_config[\"values\"]:\n",
        "            continue\n",
        "            \n",
        "        v = schwartz_config[\"values\"][value_name]\n",
        "        \n",
        "        context_parts.append(f\"\"\"\n",
        "### {value_name}\n",
        "**Core Motivation:** {v[\"core_motivation\"].strip()}\n",
        "\n",
        "**Key Behaviors (Aligned):**\n",
        "{chr(10).join(f\"- {b}\" for b in v[\"behavioral_manifestations\"][:3])}\n",
        "\n",
        "**Key Behaviors (Misaligned):**\n",
        "- Acting against the core motivation\n",
        "- Neglecting or undermining this value\n",
        "- Making choices that conflict with this value's principles\n",
        "\"\"\")\n",
        "    \n",
        "    return \"\\n\".join(context_parts)\n",
        "\n",
        "\n",
        "# Test the function\n",
        "test_rubric = build_value_rubric_context(schwartz_config)\n",
        "print(\"Sample rubric context (first 500 chars):\")\n",
        "print(test_rubric[:500] + \"...\" if len(test_rubric) > 500 else test_rubric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "judge_prompt_template = Template(\"\"\"\n",
        "You are a Judge evaluating a journal entry's alignment with Schwartz's Theory of Basic Human Values.\n",
        "\n",
        "## Your Task\n",
        "\n",
        "Evaluate how the journal entry aligns with each of the 10 Schwartz value dimensions. For each dimension, assign one of three scores:\n",
        "\n",
        "- **-1 (Misaligned):** The entry actively conflicts with this value. The person's behavior, choices, or expressed attitudes undermine or go against this value's core motivation.\n",
        "- **0 (Neutral):** The entry is irrelevant to this value, or the person's behavior maintains the status quo without clearly supporting or undermining it.\n",
        "- **+1 (Aligned):** The entry actively supports this value. The person's behavior, choices, or expressed attitudes demonstrate or advance this value's core motivation.\n",
        "\n",
        "## Important Guidelines\n",
        "\n",
        "1. **Consider the entry holistically:** A single entry may impact multiple values simultaneously. For example, working late might align with Achievement but misalign with Hedonism (rest/pleasure).\n",
        "\n",
        "2. **Look for concrete behaviors:** Base scores on what the person actually did, said, or chose, not just abstract statements about values.\n",
        "\n",
        "3. **Profile context matters:** Consider the persona's stated core values ({{ core_values }}) when evaluating alignment, but don't let it bias youâ€”evaluate what the entry actually shows.\n",
        "\n",
        "4. **Be strict with -1 and +1:** Reserve these for clear cases. Use 0 when the entry doesn't clearly relate to a value or maintains neutral status.\n",
        "\n",
        "5. **Trade-offs are normal:** It's common for an entry to align with some values while misaligning with others. This is expected and realistic.\n",
        "\n",
        "## Schwartz Value Rubrics\n",
        "\n",
        "{{ value_rubric }}\n",
        "\n",
        "## Journal Entry to Evaluate\n",
        "\n",
        "**Persona Context:**\n",
        "- Name: {{ persona_name }}\n",
        "- Age: {{ persona_age }}\n",
        "- Profession: {{ persona_profession }}\n",
        "- Culture: {{ persona_culture }}\n",
        "- Core Values (from profile): {{ core_values }}\n",
        "- Bio: {{ persona_bio }}\n",
        "\n",
        "**Journal Entry:**\n",
        "Date: {{ entry_date }}\n",
        "Content: {{ entry_content }}\n",
        "\n",
        "## Output\n",
        "\n",
        "Return a JSON object with integer scores (-1, 0, or +1) for each value dimension:\n",
        "{\n",
        "  \"self_direction\": <integer>,\n",
        "  \"stimulation\": <integer>,\n",
        "  \"hedonism\": <integer>,\n",
        "  \"achievement\": <integer>,\n",
        "  \"power\": <integer>,\n",
        "  \"security\": <integer>,\n",
        "  \"conformity\": <integer>,\n",
        "  \"tradition\": <integer>,\n",
        "  \"benevolence\": <integer>,\n",
        "  \"universalism\": <integer>\n",
        "}\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Client Setup\n",
        "\n",
        "Using OpenAI API with GPT models. You can switch to other models as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = AsyncOpenAI()\n",
        "MODEL_NAME = \"gpt-4o-mini\"  # Can switch to gpt-4o, gpt-4-turbo, etc.\n",
        "\n",
        "# Type alias for reasoning effort levels (for GPT-5 models)\n",
        "ReasoningEffort = Literal[\"minimal\", \"low\", \"medium\", \"high\"]\n",
        "\n",
        "# Default reasoning effort - only used for GPT-5 models\n",
        "DEFAULT_REASONING_EFFORT: ReasoningEffort = \"high\"\n",
        "\n",
        "\n",
        "async def generate_completion(\n",
        "    prompt: str,\n",
        "    response_format: dict | None = None,\n",
        ") -> str | None:\n",
        "    \"\"\"Generate a completion using the OpenAI API (async).\n",
        "    \n",
        "    Supports both standard Chat API and Responses API (GPT-5).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if using GPT-5 (Responses API)\n",
        "        if MODEL_NAME.startswith(\"gpt-5\"):\n",
        "            kwargs = {\n",
        "                \"model\": MODEL_NAME,\n",
        "                \"input\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"reasoning\": {\"effort\": DEFAULT_REASONING_EFFORT},\n",
        "            }\n",
        "            if response_format:\n",
        "                kwargs[\"text\"] = {\"format\": response_format}\n",
        "            response = await client.responses.create(**kwargs)\n",
        "            return response.output_text\n",
        "        else:\n",
        "            # Standard Chat API\n",
        "            kwargs = {\n",
        "                \"model\": MODEL_NAME,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            }\n",
        "            if response_format:\n",
        "                # Transform Responses API format to Chat API format\n",
        "                # Chat API expects json_schema nested under response_format\n",
        "                chat_format = {\n",
        "                    \"type\": response_format.get(\"type\", \"json_schema\"),\n",
        "                    \"json_schema\": {\n",
        "                        \"name\": response_format.get(\"name\", \"Response\"),\n",
        "                        \"schema\": response_format.get(\"schema\", {}),\n",
        "                        \"strict\": response_format.get(\"strict\", False),\n",
        "                    }\n",
        "                }\n",
        "                kwargs[\"response_format\"] = chat_format\n",
        "            response = await client.chat.completions.create(**kwargs)\n",
        "            return response.choices[0].message.content\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error generating completion: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def judge_entry(\n",
        "    entry_text: str,\n",
        "    entry_date: str,\n",
        "    persona_name: str,\n",
        "    persona_age: str,\n",
        "    persona_profession: str,\n",
        "    persona_culture: str,\n",
        "    persona_core_values: list[str],\n",
        "    persona_bio: str,\n",
        "    schwartz_config: dict,\n",
        "    max_attempts: int = 2,\n",
        ") -> tuple[AlignmentLabel | None, str]:\n",
        "    \"\"\"Judge a single journal entry and return alignment scores.\n",
        "    \n",
        "    Args:\n",
        "        entry_text: The journal entry content\n",
        "        entry_date: Date of the entry\n",
        "        persona_name: Name of the persona\n",
        "        persona_age: Age of the persona\n",
        "        persona_profession: Profession of the persona\n",
        "        persona_culture: Cultural background\n",
        "        persona_core_values: List of Schwartz values from persona profile\n",
        "        persona_bio: Persona biography\n",
        "        schwartz_config: Schwartz values configuration\n",
        "        max_attempts: Number of retry attempts for validation\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (AlignmentLabel or None, prompt used)\n",
        "    \"\"\"\n",
        "    value_rubric = build_value_rubric_context(schwartz_config)\n",
        "    \n",
        "    prompt = judge_prompt_template.render(\n",
        "        entry_content=entry_text,\n",
        "        entry_date=entry_date,\n",
        "        persona_name=persona_name,\n",
        "        persona_age=persona_age,\n",
        "        persona_profession=persona_profession,\n",
        "        persona_culture=persona_culture,\n",
        "        core_values=\", \".join(persona_core_values),\n",
        "        persona_bio=persona_bio,\n",
        "        value_rubric=value_rubric,\n",
        "    )\n",
        "    \n",
        "    last_label: AlignmentLabel | None = None\n",
        "    \n",
        "    for attempt in range(max_attempts):\n",
        "        raw_json = await generate_completion(\n",
        "            prompt, response_format=ALIGNMENT_LABEL_RESPONSE_FORMAT\n",
        "        )\n",
        "        \n",
        "        if not raw_json:\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            data = json.loads(raw_json)\n",
        "            label = AlignmentLabel(**data)\n",
        "            last_label = label\n",
        "            return label, prompt\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed to parse: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return last_label, prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n",
        "\n",
        "Load personas and journal entries. This assumes you have data from `journal_gen.ipynb` or saved elsewhere.\n",
        "\n",
        "You can either:\n",
        "1. Load from saved JSON/Parquet files\n",
        "2. Import directly from the generator notebook\n",
        "3. Load from a database\n",
        "\n",
        "For now, we'll show a simple example structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loading functions defined. Implement load_personas_and_entries() with your data source.\n"
          ]
        }
      ],
      "source": [
        "# Example data structure - replace with your actual data loading\n",
        "# This matches the output structure from journal_gen.ipynb\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class PersonaData:\n",
        "    \"\"\"Container for persona information.\"\"\"\n",
        "    persona_id: int\n",
        "    name: str\n",
        "    age: str\n",
        "    profession: str\n",
        "    culture: str\n",
        "    core_values: list[str]\n",
        "    bio: str\n",
        "\n",
        "@dataclass\n",
        "class EntryData:\n",
        "    \"\"\"Container for journal entry information.\"\"\"\n",
        "    persona_id: int\n",
        "    t_index: int\n",
        "    date: str\n",
        "    content: str\n",
        "\n",
        "@dataclass\n",
        "class JudgeResult:\n",
        "    \"\"\"Container for Judge labeling result.\"\"\"\n",
        "    persona_id: int\n",
        "    t_index: int\n",
        "    alignment_label: AlignmentLabel\n",
        "    prompt: str\n",
        "    error: Optional[str] = None\n",
        "\n",
        "\n",
        "# Example: Load data from a JSON file or database\n",
        "# For now, we'll create a simple example structure\n",
        "# Replace this with your actual data loading logic\n",
        "\n",
        "def load_personas_and_entries(data_path: str | Path) -> tuple[list[PersonaData], list[EntryData]]:\n",
        "    \"\"\"Load personas and entries from a data file.\n",
        "    \n",
        "    Replace this with your actual data loading implementation.\n",
        "    \"\"\"\n",
        "    # TODO: Implement actual data loading\n",
        "    # Example structure:\n",
        "    # {\n",
        "    #   \"personas\": [\n",
        "    #     {\n",
        "    #       \"persona_id\": 1,\n",
        "    #       \"name\": \"Yuna Park\",\n",
        "    #       \"age\": \"31\",\n",
        "    #       \"profession\": \"Parent (Stay-at-home)\",\n",
        "    #       \"culture\": \"East Asian\",\n",
        "    #       \"core_values\": [\"Benevolence\", \"Universalism\"],\n",
        "    #       \"bio\": \"...\"\n",
        "    #     }\n",
        "    #   ],\n",
        "    #   \"entries\": [\n",
        "    #     {\n",
        "    #       \"persona_id\": 1,\n",
        "    #       \"t_index\": 0,\n",
        "    #       \"date\": \"2023-10-27\",\n",
        "    #       \"content\": \"...\"\n",
        "    #     }\n",
        "    #   ]\n",
        "    # }\n",
        "    return [], []\n",
        "\n",
        "\n",
        "print(\"Data loading functions defined. Implement load_personas_and_entries() with your data source.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def judge_entry_batch(\n",
        "    personas: list[PersonaData],\n",
        "    entries: list[EntryData],\n",
        "    schwartz_config: dict,\n",
        ") -> list[JudgeResult]:\n",
        "    \"\"\"Judge multiple entries in parallel.\n",
        "    \n",
        "    Args:\n",
        "        personas: List of persona data\n",
        "        entries: List of entry data\n",
        "        schwartz_config: Schwartz values configuration\n",
        "        \n",
        "    Returns:\n",
        "        List of JudgeResult objects\n",
        "    \"\"\"\n",
        "    # Create persona lookup\n",
        "    persona_dict = {p.persona_id: p for p in personas}\n",
        "    \n",
        "    # Create tasks for parallel execution\n",
        "    tasks = []\n",
        "    for entry in entries:\n",
        "        persona = persona_dict.get(entry.persona_id)\n",
        "        if not persona:\n",
        "            continue\n",
        "            \n",
        "        task = judge_entry(\n",
        "            entry_text=entry.content,\n",
        "            entry_date=entry.date,\n",
        "            persona_name=persona.name,\n",
        "            persona_age=persona.age,\n",
        "            persona_profession=persona.profession,\n",
        "            persona_culture=persona.culture,\n",
        "            persona_core_values=persona.core_values,\n",
        "            persona_bio=persona.bio,\n",
        "            schwartz_config=schwartz_config,\n",
        "        )\n",
        "        tasks.append((entry.persona_id, entry.t_index, task))\n",
        "    \n",
        "    # Execute all tasks in parallel\n",
        "    results = await asyncio.gather(*[t[2] for t in tasks], return_exceptions=True)\n",
        "    \n",
        "    # Package results\n",
        "    judge_results = []\n",
        "    for (persona_id, t_index, _), result in zip(tasks, results):\n",
        "        if isinstance(result, Exception):\n",
        "            judge_results.append(\n",
        "                JudgeResult(\n",
        "                    persona_id=persona_id,\n",
        "                    t_index=t_index,\n",
        "                    alignment_label=None,  # type: ignore\n",
        "                    prompt=\"\",\n",
        "                    error=str(result),\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            label, prompt = result\n",
        "            judge_results.append(\n",
        "                JudgeResult(\n",
        "                    persona_id=persona_id,\n",
        "                    t_index=t_index,\n",
        "                    alignment_label=label,\n",
        "                    prompt=prompt,\n",
        "                )\n",
        "            )\n",
        "    \n",
        "    return judge_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution Example\n",
        "\n",
        "This cell shows how to run the Judge on your data. Replace the example data with your actual personas and entries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Judging 1 entries using model: gpt-4o-mini\n",
            "Personas: 1\n",
            "\n",
            "================================================================================\n",
            "Persona 1, Entry 0\n",
            "================================================================================\n",
            "Alignment Scores:\n",
            "  Self-Direction      :  0 (Neutral)\n",
            "  Stimulation         :  0 (Neutral)\n",
            "  Hedonism            : -1 (Misaligned)\n",
            "  Achievement         :  0 (Neutral)\n",
            "  Power               :  0 (Neutral)\n",
            "  Security            :  0 (Neutral)\n",
            "  Conformity          : -1 (Misaligned)\n",
            "  Tradition           :  0 (Neutral)\n",
            "  Benevolence         : +1 (Aligned)\n",
            "  Universalism        : +1 (Aligned)\n",
            "\n",
            "Vector representation: [0, 0, -1, 0, 0, 0, -1, 0, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "# Example: Create sample data for testing\n",
        "# Replace this with your actual data loading\n",
        "\n",
        "example_personas = [\n",
        "    PersonaData(\n",
        "        persona_id=1,\n",
        "        name=\"Yuna Park\",\n",
        "        age=\"31\",\n",
        "        profession=\"Parent (Stay-at-home)\",\n",
        "        culture=\"East Asian\",\n",
        "        core_values=[\"Benevolence\", \"Universalism\"],\n",
        "        bio=\"Yuna left her preschool job after her daughter was born and now spends her days managing naps and pediatrician appointments.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "example_entries = [\n",
        "    EntryData(\n",
        "        persona_id=1,\n",
        "        t_index=0,\n",
        "        date=\"2023-10-27\",\n",
        "        content=\"The food pantry shift ran late again. I'm exhausted but it felt good to help. My partner is frustrated I keep saying yes to these commitments.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Run the Judge\n",
        "print(f\"Judging {len(example_entries)} entries using model: {MODEL_NAME}\")\n",
        "print(f\"Personas: {len(example_personas)}\")\n",
        "\n",
        "results = await judge_entry_batch(\n",
        "    personas=example_personas,\n",
        "    entries=example_entries,\n",
        "    schwartz_config=schwartz_config,\n",
        ")\n",
        "\n",
        "# Display results\n",
        "for result in results:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Persona {result.persona_id}, Entry {result.t_index}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    if result.error:\n",
        "        print(f\"ERROR: {result.error}\")\n",
        "    elif result.alignment_label:\n",
        "        print(\"Alignment Scores:\")\n",
        "        label = result.alignment_label\n",
        "        for i, value_name in enumerate(SCHWARTZ_VALUE_ORDER):\n",
        "            score = label.to_vector()[i]\n",
        "            score_str = {1: \"+1 (Aligned)\", 0: \" 0 (Neutral)\", -1: \"-1 (Misaligned)\"}[score]\n",
        "            print(f\"  {value_name:20s}: {score_str}\")\n",
        "        \n",
        "        print(f\"\\nVector representation: {label.to_vector()}\")\n",
        "    else:\n",
        "        print(\"No label generated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n",
        "\n",
        "Save the Judge labels to a format suitable for training the Critic (e.g., Parquet, JSON, or database).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_judge_labels(results: list[JudgeResult], output_path: str | Path) -> None:\n",
        "    \"\"\"Save Judge labels to a Parquet file for downstream processing.\n",
        "    \n",
        "    This creates a table matching the JudgeLabel schema from VIF_05.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for result in results:\n",
        "        if result.error or not result.alignment_label:\n",
        "            continue\n",
        "            \n",
        "        alignment_vector = result.alignment_label.to_vector()\n",
        "        \n",
        "        row = {\n",
        "            \"persona_id\": result.persona_id,\n",
        "            \"t_index\": result.t_index,\n",
        "            \"alignment_vector\": alignment_vector,  # List of 10 integers\n",
        "        }\n",
        "        \n",
        "        # Also include individual scores for easier inspection\n",
        "        for i, value_name in enumerate(SCHWARTZ_VALUE_ORDER):\n",
        "            row[f\"alignment_{value_name.lower().replace('-', '_')}\"] = alignment_vector[i]\n",
        "        \n",
        "        rows.append(row)\n",
        "    \n",
        "    if rows:\n",
        "        df = pl.DataFrame(rows)\n",
        "        df.write_parquet(output_path)\n",
        "        print(f\"Saved {len(rows)} Judge labels to {output_path}\")\n",
        "        print(f\"\\nSchema:\")\n",
        "        print(df.schema)\n",
        "        print(f\"\\nSample rows:\")\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"No valid labels to save\")\n",
        "\n",
        "\n",
        "# Example: Save results\n",
        "# output_path = Path(\"data/judge_labels.parquet\")\n",
        "# save_judge_labels(results, output_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
